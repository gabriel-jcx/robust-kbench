import torch
import torch.nn as nn
import torch.nn.functional as F


def forward_fn(
    x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float
) -> torch.Tensor:
    """
    Functional implementation of LayerNorm. Layer normalization computes the mean and variance across the last N dimensions specified by normalized_shape. For an input x, the formula is:

        y = (x - E[x]) / sqrt(Var[x] + eps) * weight + bias

    where E[x] and Var[x] are computed across the normalized dimensions. The weight and bias parameters are learnable affine transformations applied after normalization.

    Args:
        x (torch.Tensor): Input tensor of shape (*, normalized_shape).
        weight (torch.Tensor): Weight tensor of shape (normalized_shape).
        bias (torch.Tensor): Bias tensor of shape (normalized_shape).
        eps (float): Epsilon parameter for numerical stability.

    Returns:
        torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.
    """
    assert x.is_cuda, f"x is on {x.device}"
    assert weight.is_cuda, f"weight is on {weight.device}"
    assert bias.is_cuda, f"bias is on {bias.device}"
    # Get the normalized shape from the weight tensor
    normalized_shape = tuple(x.shape[-len(weight.shape) :])
    return F.layer_norm(
        x, normalized_shape=normalized_shape, weight=weight, bias=bias, eps=eps
    )


class Model(nn.Module):
    """
    Simple model that performs Layer Normalization.
    """

    def __init__(
        self,
        num_features: int,
        dim1: int,
        dim2: int,
        eps: float = 1e-5,
        init_method: str = "standard",
        device: str = "cuda",
    ):
        """
        Initializes the LayerNorm layer parameters.

        Args:
            normalized_shape (tuple): Shape of the input tensor to be normalized.
        """
        super(Model, self).__init__()
        self.normalized_shape = (num_features, dim1, dim2)
        self.eps = eps

        if init_method == "standard":
            weight = torch.ones(self.normalized_shape,device = device)
            bias = torch.zeros(self.normalized_shape,device = device)
        elif init_method == "random":
            weight = torch.randn(self.normalized_shape,device = device)
            bias = torch.randn(self.normalized_shape,device = device)

        self.weight = nn.Parameter(weight)
        self.bias = nn.Parameter(bias)

    def forward(self, x: torch.Tensor, fn=forward_fn) -> torch.Tensor:
        """
        Applies Layer Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (*, normalized_shape).
            fn: Function to apply (defaults to forward_fn)

        Returns:
            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.
        """

        # assert x.is_cuda, f"x is on {x.device}"
        # assert self.weight.is_cuda, f"weight is on {self.weight.device}"
        # assert self.bias.is_cuda, f"bias is on {self.bias.device}"
        if fn is None:
            fn = forward_fn

        return fn(x, self.weight, self.bias, self.eps)


def get_inputs(
    batch_size: int = 16, num_features: int = 64, dim1: int = 256, dim2: int = 256
):
    x = torch.randn(batch_size, num_features, dim1, dim2)
    return [x]


input_names = ["x"]
